id: openai-assistant-production
namespace: serena.energia
description: "Workflow de Produ√ß√£o - OpenAI Assistants API para Conversa√ß√£o com Leads"

labels:
  version: "1.0.0"
  environment: "production"
  system: "serena-qualifier"
  framework: "openai_assistant"
  architecture: "production_ready"

# Trigger Webhook para receber mensagens do WhatsApp
triggers:
  - id: assistant-conversation-webhook
    type: io.kestra.plugin.core.trigger.Webhook
    key: "openai_assistant_webhook"

inputs:
  - id: lead_phone
    type: STRING
    required: false
  - id: lead_message
    type: STRING
    required: false
  - id: message_type
    type: STRING
    required: false
    defaults: "text"
  - id: message_id
    type: STRING
    required: false
  - id: media_id
    type: STRING
    required: false
  - id: conversation_id
    type: STRING
    required: false

tasks:
  # TASK 1: Obter ou Criar Thread
  - id: get-or-create-thread
    type: io.kestra.plugin.scripts.python.Script
    description: "Gerenciar Thread OpenAI por phone_number"
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
      image: "serena-runner:latest"
      pullPolicy: "NEVER"
      networkMode: "serena-qualifier_kestra-network"
      fileHandlingStrategy: VOLUME
    env:
      LEAD_PHONE: "{{ inputs.lead_phone | default(trigger.body.lead_phone | default('')) }}"
    script: |
      # Carregar vari√°veis de ambiente
      from dotenv import load_dotenv
      load_dotenv('/app/.env')
      
      # Importar depend√™ncias
      import os
      import sys
      import json
      from openai import OpenAI
      from supabase import create_client, Client
      
      phone = os.getenv("LEAD_PHONE")
      print(f"üßµ THREAD MANAGER: Phone {phone}")
      
      try:
          # Inicializar clientes
          openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
          supabase_url = os.getenv('SUPABASE_URL')
          supabase_key = os.getenv('SUPABASE_ANON_KEY')
          supabase = create_client(supabase_url, supabase_key)
          
          # Buscar thread existente
          result = supabase.table('conversations').select('thread_id').eq('phone_number', phone).execute()
          
          if result.data:
              thread_id = result.data[0]['thread_id']
              print(f"‚úÖ Thread existente: {thread_id}")
          else:
              # Criar nova thread
              thread = openai_client.beta.threads.create()
              thread_id = thread.id
              
              # Salvar no Supabase
              supabase.table('conversations').insert({
                  'phone_number': phone,
                  'thread_id': thread_id,
                  'status': 'active'
              }).execute()
              
              print(f"‚úÖ Nova thread criada: {thread_id}")
          
          print('::' + json.dumps({"outputs": {"thread_id": thread_id, "phone": phone, "success": True}}) + '::')
          
      except Exception as e:
          print(f"‚ùå Erro: {str(e)}")
          print('::' + json.dumps({"outputs": {"thread_id": None, "success": False, "error": str(e)}}) + '::')

  # TASK 2: Adicionar Mensagem √† Thread
  - id: add-message-to-thread
    type: io.kestra.plugin.scripts.python.Script
    description: "Adicionar mensagem do lead √† thread"
    runIf: "{{ outputs['get-or-create-thread'].vars.success == true }}"
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
      image: "serena-runner:latest"
      pullPolicy: "NEVER"
      networkMode: "serena-qualifier_kestra-network"
      fileHandlingStrategy: VOLUME
    env:
      THREAD_ID: "{{ outputs['get-or-create-thread'].vars.thread_id }}"
      LEAD_MESSAGE: "{{ inputs.lead_message | default(trigger.body.lead_message | default('')) }}"
      MESSAGE_TYPE: "{{ inputs.message_type | default(trigger.body.message_type | default('text')) }}"
      MEDIA_ID: "{{ inputs.media_id | default(trigger.body.media_id | default('')) }}"
    script: |
      from dotenv import load_dotenv
      load_dotenv('/app/.env')
      
      import os
      import json
      from openai import OpenAI
      
      thread_id = os.getenv("THREAD_ID")
      message = os.getenv("LEAD_MESSAGE")
      msg_type = os.getenv("MESSAGE_TYPE")
      media_id = os.getenv("MEDIA_ID")
      
      print(f"üí¨ ADD MESSAGE: Thread {thread_id}")
      print(f"üìù Mensagem: {message}")
      print(f"üìé Tipo: {msg_type}")
      
      try:
          openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
          
          # Preparar conte√∫do da mensagem
          if msg_type in ["image", "document"] and media_id:
              content = f"[M√çDIA: {msg_type}] {message}\nMedia ID: {media_id}"
          else:
              content = message
          
          # Adicionar mensagem √† thread
          message_obj = openai_client.beta.threads.messages.create(
              thread_id=thread_id,
              role="user",
              content=content
          )
          
          print(f"‚úÖ Mensagem adicionada: {message_obj.id}")
          print('::' + json.dumps({"outputs": {"message_id": message_obj.id, "success": True}}) + '::')
          
      except Exception as e:
          print(f"‚ùå Erro: {str(e)}")
          print('::' + json.dumps({"outputs": {"message_id": None, "success": False, "error": str(e)}}) + '::')

  # TASK 3: Criar Run do Assistant
  - id: create-assistant-run
    type: io.kestra.plugin.scripts.python.Script
    description: "Criar run do OpenAI Assistant"
    runIf: "{{ outputs['add-message-to-thread'].vars.success == true }}"
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
      image: "serena-runner:latest"
      pullPolicy: "NEVER"
      networkMode: "serena-qualifier_kestra-network"
      fileHandlingStrategy: VOLUME
    env:
      THREAD_ID: "{{ outputs['get-or-create-thread'].vars.thread_id }}"
    script: |
      from dotenv import load_dotenv
      load_dotenv('/app/.env')
      
      import os
      import json
      from openai import OpenAI
      
      thread_id = os.getenv("THREAD_ID")
      assistant_id = os.getenv("OPENAI_ASSISTANT_ID")
      
      print(f"ü§ñ CREATE RUN: Assistant {assistant_id}")
      print(f"üßµ Thread: {thread_id}")
      
      try:
          openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
          
          # Criar run
          run = openai_client.beta.threads.runs.create(
              thread_id=thread_id,
              assistant_id=assistant_id
          )
          
          print(f"‚úÖ Run criada: {run.id}")
          print(f"üìä Status inicial: {run.status}")
          
          print('::' + json.dumps({"outputs": {"run_id": run.id, "status": run.status, "success": True}}) + '::')
          
      except Exception as e:
          print(f"‚ùå Erro: {str(e)}")
          print('::' + json.dumps({"outputs": {"run_id": None, "success": False, "error": str(e)}}) + '::')

  # TASK 4: Polling do Status da Run
  - id: poll-run-status
    type: io.kestra.plugin.scripts.python.Script
    description: "Polling do status da run com tratamento de function calls"
    runIf: "{{ outputs['create-assistant-run'].vars.success == true }}"
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
      image: "serena-runner:latest"
      pullPolicy: "NEVER"
      networkMode: "serena-qualifier_kestra-network"
      fileHandlingStrategy: VOLUME
    env:
      THREAD_ID: "{{ outputs['get-or-create-thread'].vars.thread_id }}"
      RUN_ID: "{{ outputs['create-assistant-run'].vars.run_id }}"
    script: |
      from dotenv import load_dotenv
      load_dotenv('/app/.env')
      
      import os
      import json
      import time
      from openai import OpenAI
      
      # Importar ferramentas do projeto
      import sys
      sys.path.append('/app')
      from scripts.serena_agent.tools.rag_tool import rag_tool_function
      from scripts.serena_agent.tools.serena_api_tool import serena_api_tool_function
      from scripts.serena_agent.tools.ocr_tool import ocr_tool_function
      
      thread_id = os.getenv("THREAD_ID")
      run_id = os.getenv("RUN_ID")
      
      print(f"üîÑ POLLING: Run {run_id}")
      
      try:
          openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
          max_attempts = 20
          final_status = None
          
          for attempt in range(max_attempts):
              print(f"üîÑ Tentativa {attempt + 1}/{max_attempts}")
              
              # Verificar status
              run = openai_client.beta.threads.runs.retrieve(
                  thread_id=thread_id,
                  run_id=run_id
              )
              
              print(f"üìä Status: {run.status}")
              
              if run.status == "completed":
                  final_status = "completed"
                  print("‚úÖ Run completada!")
                  break
              elif run.status == "requires_action":
                  print("üîß Function calls necess√°rios")
                  
                  # Processar function calls
                  tool_outputs = []
                  for tool_call in run.required_action.submit_tool_outputs.tool_calls:
                      function_name = tool_call.function.name
                      arguments = json.loads(tool_call.function.arguments)
                      
                      print(f"üõ†Ô∏è Executando: {function_name}")
                      
                      # Executar fun√ß√£o correspondente
                      if function_name == "search_knowledge_base":
                          result = rag_tool_function(arguments)
                      elif function_name == "get_energy_plans":
                          result = serena_api_tool_function(arguments)
                      elif function_name == "extract_amount_from_image":
                          result = ocr_tool_function(arguments)
                      else:
                          result = {"error": f"Fun√ß√£o {function_name} n√£o encontrada"}
                      
                      tool_outputs.append({
                          "tool_call_id": tool_call.id,
                          "output": json.dumps(result)
                      })
                  
                  # Submeter resultados
                  openai_client.beta.threads.runs.submit_tool_outputs(
                      thread_id=thread_id,
                      run_id=run_id,
                      tool_outputs=tool_outputs
                  )
                  
                  print("‚úÖ Tool outputs submetidos")
                  
              elif run.status in ["failed", "cancelled", "expired"]:
                  final_status = run.status
                  print(f"‚ùå Run falhou: {run.status}")
                  break
              
              if attempt < max_attempts - 1:
                  time.sleep(3)
          
          print('::' + json.dumps({"outputs": {"final_status": final_status, "attempts": attempt + 1, "success": final_status == "completed"}}) + '::')
          
      except Exception as e:
          print(f"‚ùå Erro: {str(e)}")
          print('::' + json.dumps({"outputs": {"final_status": "error", "success": False, "error": str(e)}}) + '::')

  # TASK 5: Obter Resposta Final
  - id: get-assistant-response
    type: io.kestra.plugin.scripts.python.Script
    description: "Obter resposta final do Assistant"
    runIf: "{{ outputs['poll-run-status'].vars.success == true }}"
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
      image: "serena-runner:latest"
      pullPolicy: "NEVER"
      networkMode: "serena-qualifier_kestra-network"
      fileHandlingStrategy: VOLUME
    env:
      THREAD_ID: "{{ outputs['get-or-create-thread'].vars.thread_id }}"
    script: |
      from dotenv import load_dotenv
      load_dotenv('/app/.env')
      
      import os
      import json
      from openai import OpenAI
      
      thread_id = os.getenv("THREAD_ID")
      
      print(f"üì• GET RESPONSE: Thread {thread_id}")
      
      try:
          openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
          
          # Obter mensagens da thread
          messages = openai_client.beta.threads.messages.list(
              thread_id=thread_id,
              order="desc",
              limit=1
          )
          
          if messages.data:
              assistant_message = messages.data[0]
              response_text = assistant_message.content[0].text.value
              
              print(f"ü§ñ Resposta: {response_text[:200]}...")
              
              print('::' + json.dumps({"outputs": {"response": response_text, "message_id": assistant_message.id, "success": True}}) + '::')
          else:
              print("‚ùå Nenhuma resposta encontrada")
              print('::' + json.dumps({"outputs": {"response": None, "success": False, "error": "No response found"}}) + '::')
          
      except Exception as e:
          print(f"‚ùå Erro: {str(e)}")
          print('::' + json.dumps({"outputs": {"response": None, "success": False, "error": str(e)}}) + '::')

  # TASK 6: Enviar Resposta via WhatsApp
  - id: send-whatsapp-response
    type: io.kestra.plugin.scripts.python.Script
    description: "Enviar resposta do Assistant via WhatsApp"
    runIf: "{{ outputs['get-assistant-response'].vars.success == true }}"
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
      image: "serena-runner:latest"
      pullPolicy: "NEVER"
      networkMode: "serena-qualifier_kestra-network"
      fileHandlingStrategy: VOLUME
    env:
      LEAD_PHONE: "{{ outputs['get-or-create-thread'].vars.phone }}"
      ASSISTANT_RESPONSE: "{{ outputs['get-assistant-response'].vars.response }}"
    script: |
      from dotenv import load_dotenv
      load_dotenv('/app/.env')
      
      import os
      import json
      import requests
      
      phone = os.getenv("LEAD_PHONE")
      response_text = os.getenv("ASSISTANT_RESPONSE")
      
      print(f"üì± WHATSAPP: Enviando para {phone}")
      print(f"üí¨ Mensagem: {response_text[:100]}...")
      
      try:
          # Enviar via WhatsApp API
          whatsapp_url = "http://whatsapp-service:3000/whatsapp/send_text"
          
          payload = {
              "phone_number": phone,
              "message": response_text
          }
          
          response = requests.post(whatsapp_url, json=payload, timeout=30)
          
          if response.status_code == 200:
              print("‚úÖ Mensagem enviada com sucesso!")
              result = {"success": True, "status_code": response.status_code}
          else:
              print(f"‚ùå Erro ao enviar: {response.status_code}")
              result = {"success": False, "status_code": response.status_code, "error": response.text}
          
          print('::' + json.dumps({"outputs": result}) + '::')
          
      except Exception as e:
          print(f"‚ùå Erro: {str(e)}")
          print('::' + json.dumps({"outputs": {"success": False, "error": str(e)}}) + '::')

outputs:
  - id: conversation_summary
    type: JSON
    value: |
      {
        "phone": "{{ outputs['get-or-create-thread'].vars.phone }}",
        "thread_id": "{{ outputs['get-or-create-thread'].vars.thread_id }}",
        "message_added": {{ outputs['add-message-to-thread'].vars.success | default(false) }},
        "run_created": {{ outputs['create-assistant-run'].vars.success | default(false) }},
        "run_id": "{{ outputs['create-assistant-run'].vars.run_id }}",
        "final_status": "{{ outputs['poll-run-status'].vars.final_status }}",
        "assistant_response": "{{ outputs['get-assistant-response'].vars.response }}",
        "whatsapp_sent": {{ outputs['send-whatsapp-response'].vars.success | default(false) }},
        "overall_success": {{ (outputs['send-whatsapp-response'].vars.success == true) | default(false) }}
      }
  
  - id: success
    type: BOOLEAN
    value: "{{ outputs['send-whatsapp-response'].vars.success == true }}"